{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install mne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Qt5Agg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Overview of MEG/EEG analysis with MNE-Python\n",
    "\n",
    "This tutorial covers the basic EEG/MEG pipeline for event-related analysis:\n",
    "loading data, epoching, averaging, plotting, and estimating cortical activity\n",
    "from sensor data. It introduces the core MNE-Python data structures\n",
    "`~mne.io.Raw`, `~mne.Epochs`, `~mne.Evoked`, and `~mne.SourceEstimate`, and\n",
    "covers a lot of ground fairly quickly (at the expense of depth). Subsequent\n",
    "tutorials address each of these topics in greater detail.\n",
    "\n",
    "We begin by importing the necessary Python modules:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "MNE-Python data structures are based around the FIF file format from\n",
    "Neuromag, but there are reader functions for `a wide variety of other\n",
    "data formats <data-formats>`. MNE-Python also has interfaces to a\n",
    "variety of `publicly available datasets <datasets>`, which MNE-Python\n",
    "can download and manage for you.\n",
    "\n",
    "We'll start this tutorial by loading one of the example datasets (called\n",
    "\"`sample-dataset`\"), which contains EEG and MEG data from one subject\n",
    "performing an audiovisual experiment, along with structural MRI scans for\n",
    "that subject. The `mne.datasets.sample.data_path` function will automatically\n",
    "download the dataset if it isn't found in one of the expected locations, then\n",
    "return the directory path to the dataset (see the documentation of\n",
    "`~mne.datasets.sample.data_path` for a list of places it checks before\n",
    "downloading). Note also that for this tutorial to run smoothly on our\n",
    "servers, we're using a filtered and downsampled version of the data\n",
    "(:file:`sample_audvis_filt-0-40_raw.fif`), but an unfiltered version\n",
    "(:file:`sample_audvis_raw.fif`) is also included in the sample dataset and\n",
    "could be substituted here when running the tutorial locally.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/isaac/Library/CloudStorage/OneDrive-인하대학교/python_project/MNE/mne_tutorial/01.intro'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/isaac/Library/CloudStorage/OneDrive-인하대학교/python_project/MNE/mne_data/MNE-sample-data\n"
     ]
    }
   ],
   "source": [
    "print(mne.datasets.sample.data_path())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening raw data file /Users/isaac/Library/CloudStorage/OneDrive-인하대학교/python_project/MNE/mne_data/MNE-sample-data/MEG/sample/sample_audvis_filt-0-40_raw.fif...\n",
      "    Read a total of 4 projection items:\n",
      "        PCA-v1 (1 x 102)  idle\n",
      "        PCA-v2 (1 x 102)  idle\n",
      "        PCA-v3 (1 x 102)  idle\n",
      "        Average EEG reference (1 x 60)  idle\n",
      "    Range : 6450 ... 48149 =     42.956 ...   320.665 secs\n",
      "Ready.\n"
     ]
    }
   ],
   "source": [
    "sample_data_folder = mne.datasets.sample.data_path()\n",
    "sample_data_raw_file = (sample_data_folder / 'MEG' / 'sample' /\n",
    "                        'sample_audvis_filt-0-40_raw.fif')\n",
    "raw = mne.io.read_raw_fif(sample_data_raw_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, `~mne.io.read_raw_fif` displays some information about the file\n",
    "it's loading; for example, here it tells us that there are four \"projection\n",
    "items\" in the file along with the recorded data; those are :term:`SSP\n",
    "projectors <projector>` calculated to remove environmental noise from the MEG\n",
    "signals, plus a projector to mean-reference the EEG channels; these are\n",
    "discussed in the tutorial `tut-projectors-background`. In addition to\n",
    "the information displayed during loading, you can get a glimpse of the basic\n",
    "details of a `~mne.io.Raw` object by printing it; even more is available by\n",
    "printing its ``info`` attribute (a `dictionary-like object <mne.Info>` that\n",
    "is preserved across `~mne.io.Raw`, `~mne.Epochs`, and `~mne.Evoked` objects).\n",
    "The ``info`` data structure keeps track of channel locations, applied\n",
    "filters, projectors, etc. Notice especially the ``chs`` entry, showing that\n",
    "MNE-Python detects different sensor types and handles each appropriately. See\n",
    "`tut-info-class` for more on the `~mne.Info` class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Raw | sample_audvis_filt-0-40_raw.fif, 376 x 41700 (277.7 s), ~3.3 MB, data not loaded>\n",
      "<Info | 15 non-empty values\n",
      " bads: 2 items (MEG 2443, EEG 053)\n",
      " ch_names: MEG 0113, MEG 0112, MEG 0111, MEG 0122, MEG 0123, MEG 0121, MEG ...\n",
      " chs: 204 Gradiometers, 102 Magnetometers, 9 Stimulus, 60 EEG, 1 EOG\n",
      " custom_ref_applied: False\n",
      " dev_head_t: MEG device -> head transform\n",
      " dig: 146 items (3 Cardinal, 4 HPI, 61 EEG, 78 Extra)\n",
      " file_id: 4 items (dict)\n",
      " highpass: 0.1 Hz\n",
      " hpi_meas: 1 item (list)\n",
      " hpi_results: 1 item (list)\n",
      " lowpass: 40.0 Hz\n",
      " meas_date: 2002-12-03 19:01:10 UTC\n",
      " meas_id: 4 items (dict)\n",
      " nchan: 376\n",
      " projs: PCA-v1: off, PCA-v2: off, PCA-v3: off, Average EEG reference: off\n",
      " sfreq: 150.2 Hz\n",
      ">\n"
     ]
    }
   ],
   "source": [
    "print(raw)\n",
    "print(raw.info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`~mne.io.Raw` objects also have several built-in plotting methods; here we\n",
    "show the power spectral density (PSD) for each sensor type with\n",
    "`~mne.io.Raw.plot_psd`, as well as a plot of the raw sensor traces with\n",
    "`~mne.io.Raw.plot`. In the PSD plot, we'll only plot frequencies below 50 Hz\n",
    "(since our data are low-pass filtered at 40 Hz). In interactive Python\n",
    "sessions, `~mne.io.Raw.plot` is interactive and allows scrolling, scaling,\n",
    "bad channel marking, annotations, projector toggling, etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: plot_psd() is a legacy function. New code should use .compute_psd().plot().\n",
      "Effective window size : 1.705 (s)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<MNEBrowseFigure size 1600x1600 with 5 Axes>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels marked as bad:\n",
      "['MEG 2443', 'EEG 053']\n"
     ]
    }
   ],
   "source": [
    "raw.plot_psd(fmax=50)\n",
    "raw.plot(duration=5, n_channels=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "MNE-Python supports a variety of preprocessing approaches and techniques\n",
    "(maxwell filtering, signal-space projection, independent components analysis,\n",
    "filtering, downsampling, etc); see the full list of capabilities in the\n",
    ":mod:`mne.preprocessing` and :mod:`mne.filter` submodules. Here we'll clean\n",
    "up our data by performing independent components analysis\n",
    "(`~mne.preprocessing.ICA`); for brevity we'll skip the steps that helped us\n",
    "determined which components best capture the artifacts (see\n",
    "`tut-artifact-ica` for a detailed walk-through of that process).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting ICA to data using 364 channels (please be patient, this may take a while)\n",
      "Selecting by number: 20 components\n",
      "Fitting ICA took 10.1s.\n",
      "    Using multitaper spectrum estimation with 7 DPSS windows\n",
      "Not setting metadata\n",
      "138 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "138 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Channels marked as bad:\n",
      "['MEG 2443', 'EEG 053', 'MEG 0113']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<Figure size 1400x1200 with 6 Axes>, <Figure size 1400x1200 with 6 Axes>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set up and fit the ICA\n",
    "ica = mne.preprocessing.ICA(n_components=20, random_state=97, max_iter=800)\n",
    "ica.fit(raw)\n",
    "ica.exclude = [1, 2]  # details on how we picked these are omitted here\n",
    "ica.plot_properties(raw, picks=ica.exclude)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we're confident about which component(s) we want to remove, we pass them\n",
    "as the ``exclude`` parameter and then apply the ICA to the raw signal. The\n",
    "`~mne.preprocessing.ICA.apply` method requires the raw data to be loaded into\n",
    "memory (by default it's only read from disk as-needed), so we'll use\n",
    "`~mne.io.Raw.load_data` first. We'll also make a copy of the `~mne.io.Raw`\n",
    "object so we can compare the signal before and after artifact removal\n",
    "side-by-side:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 0 ... 41699  =      0.000 ...   277.709 secs...\n",
      "Applying ICA to Raw instance\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (363,41700) (364,1) (363,41700) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m orig_raw \u001b[38;5;241m=\u001b[39m raw\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m      2\u001b[0m raw\u001b[38;5;241m.\u001b[39mload_data()\n\u001b[0;32m----> 3\u001b[0m \u001b[43mica\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# show some frontal channels to clearly illustrate the artifact removal\u001b[39;00m\n\u001b[1;32m      6\u001b[0m chs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMEG 0111\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMEG 0121\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMEG 0131\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMEG 0211\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMEG 0221\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMEG 0231\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      7\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMEG 0311\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMEG 0321\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMEG 0331\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMEG 1511\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMEG 1521\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMEG 1531\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      8\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEEG 001\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEEG 002\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEEG 003\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEEG 004\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEEG 005\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEEG 006\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      9\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEEG 007\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEEG 008\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m<decorator-gen-450>:12\u001b[0m, in \u001b[0;36mapply\u001b[0;34m(self, inst, include, exclude, n_pca_components, start, stop, on_baseline, verbose)\u001b[0m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/mne/lib/python3.10/site-packages/mne/preprocessing/ica.py:1984\u001b[0m, in \u001b[0;36mICA.apply\u001b[0;34m(self, inst, include, exclude, n_pca_components, start, stop, on_baseline, verbose)\u001b[0m\n\u001b[1;32m   1981\u001b[0m             _on_missing(on_baseline, msg, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mon_baseline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1983\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mApplying ICA to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkind\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m instance\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1984\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1985\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reapply_baseline:\n\u001b[1;32m   1986\u001b[0m     out\u001b[38;5;241m.\u001b[39mapply_baseline(inst\u001b[38;5;241m.\u001b[39mbaseline)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/mne/lib/python3.10/site-packages/mne/preprocessing/ica.py:2006\u001b[0m, in \u001b[0;36mICA._apply_raw\u001b[0;34m(self, raw, include, exclude, n_pca_components, start, stop)\u001b[0m\n\u001b[1;32m   2002\u001b[0m picks \u001b[38;5;241m=\u001b[39m pick_types(raw\u001b[38;5;241m.\u001b[39minfo, meg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, include\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mch_names,\n\u001b[1;32m   2003\u001b[0m                    exclude\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbads\u001b[39m\u001b[38;5;124m'\u001b[39m, ref_meg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   2005\u001b[0m data \u001b[38;5;241m=\u001b[39m raw[picks, start:stop][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m-> 2006\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pick_sources\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_pca_components\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2008\u001b[0m raw[picks, start:stop] \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m   2009\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m raw\n",
      "File \u001b[0;32m/opt/miniconda3/envs/mne/lib/python3.10/site-packages/mne/preprocessing/ica.py:2063\u001b[0m, in \u001b[0;36mICA._pick_sources\u001b[0;34m(self, data, include, exclude, n_pca_components)\u001b[0m\n\u001b[1;32m   2061\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_pca_components \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2062\u001b[0m     n_pca_components \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_pca_components\n\u001b[0;32m-> 2063\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pre_whiten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2064\u001b[0m exclude \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_exclude(exclude)\n\u001b[1;32m   2065\u001b[0m _n_pca_comp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_n_pca_components(n_pca_components)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/mne/lib/python3.10/site-packages/mne/preprocessing/ica.py:770\u001b[0m, in \u001b[0;36mICA._pre_whiten\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    768\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_proj(data, log_suffix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(pre-whitener application)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    769\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnoise_cov \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 770\u001b[0m     data \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_whitener_\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_whitener_ \u001b[38;5;241m@\u001b[39m data\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (363,41700) (364,1) (363,41700) "
     ]
    }
   ],
   "source": [
    "orig_raw = raw.copy()\n",
    "raw.load_data()\n",
    "ica.apply(raw)\n",
    "\n",
    "# show some frontal channels to clearly illustrate the artifact removal\n",
    "chs = ['MEG 0111', 'MEG 0121', 'MEG 0131', 'MEG 0211', 'MEG 0221', 'MEG 0231',\n",
    "       'MEG 0311', 'MEG 0321', 'MEG 0331', 'MEG 1511', 'MEG 1521', 'MEG 1531',\n",
    "       'EEG 001', 'EEG 002', 'EEG 003', 'EEG 004', 'EEG 005', 'EEG 006',\n",
    "       'EEG 007', 'EEG 008']\n",
    "chan_idxs = [raw.ch_names.index(ch) for ch in chs]\n",
    "orig_raw.plot(order=chan_idxs, start=12, duration=4)\n",
    "raw.plot(order=chan_idxs, start=12, duration=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Detecting experimental events\n",
    "\n",
    "The sample dataset includes several :term:`\"STIM\" channels <stim channel>`\n",
    "that recorded electrical signals sent from the stimulus delivery computer (as\n",
    "brief DC shifts / squarewave pulses). These pulses (often called \"triggers\")\n",
    "are used in this dataset to mark experimental events: stimulus onset,\n",
    "stimulus type, and participant response (button press). The individual STIM\n",
    "channels are combined onto a single channel, in such a way that voltage\n",
    "levels on that channel can be unambiguously decoded as a particular event\n",
    "type. On older Neuromag systems (such as that used to record the sample data)\n",
    "this summation channel was called ``STI 014``, so we can pass that channel\n",
    "name to the `mne.find_events` function to recover the timing and identity of\n",
    "the stimulus events.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319 events found\n",
      "Event IDs: [ 1  2  3  4  5 32]\n",
      "[[6994    0    2]\n",
      " [7086    0    3]\n",
      " [7192    0    1]\n",
      " [7304    0    4]\n",
      " [7413    0    2]]\n"
     ]
    }
   ],
   "source": [
    "events = mne.find_events(raw, stim_channel='STI 014')\n",
    "print(events[:5])  # show the first 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting events array is an ordinary 3-column :class:`NumPy array\n",
    "<numpy.ndarray>`, with sample number in the first column and integer event ID\n",
    "in the last column; the middle column is usually ignored. Rather than keeping\n",
    "track of integer event IDs, we can provide an *event dictionary* that maps\n",
    "the integer IDs to experimental conditions or events. In this dataset, the\n",
    "mapping looks like this:\n",
    "\n",
    "\n",
    "+----------+----------------------------------------------------------+\n",
    "| Event ID | Condition                                                |\n",
    "+==========+==========================================================+\n",
    "| 1        | auditory stimulus (tone) to the left ear                 |\n",
    "+----------+----------------------------------------------------------+\n",
    "| 2        | auditory stimulus (tone) to the right ear                |\n",
    "+----------+----------------------------------------------------------+\n",
    "| 3        | visual stimulus (checkerboard) to the left visual field  |\n",
    "+----------+----------------------------------------------------------+\n",
    "| 4        | visual stimulus (checkerboard) to the right visual field |\n",
    "+----------+----------------------------------------------------------+\n",
    "| 5        | smiley face (catch trial)                                |\n",
    "+----------+----------------------------------------------------------+\n",
    "| 32       | subject button press                                     |\n",
    "+----------+----------------------------------------------------------+\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "event_dict = {'auditory/left': 1, 'auditory/right': 2, 'visual/left': 3,\n",
    "              'visual/right': 4, 'smiley': 5, 'buttonpress': 32}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Event dictionaries like this one are used when extracting epochs from\n",
    "continuous data; the ``/`` character in the dictionary keys allows pooling\n",
    "across conditions by requesting partial condition descriptors (i.e.,\n",
    "requesting ``'auditory'`` will select all epochs with Event IDs 1 and 2;\n",
    "requesting ``'left'`` will select all epochs with Event IDs 1 and 3). An\n",
    "example of this is shown in the next section. There is also a convenient\n",
    "`~mne.viz.plot_events` function for visualizing the distribution of events\n",
    "across the duration of the recording (to make sure event detection worked as\n",
    "expected). Here we'll also make use of the `~mne.Info` attribute to get the\n",
    "sampling frequency of the recording (so our x-axis will be in seconds instead\n",
    "of in samples).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig = mne.viz.plot_events(events, event_id=event_dict, sfreq=raw.info['sfreq'],\n",
    "                          first_samp=raw.first_samp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For paradigms that are not event-related (e.g., analysis of resting-state\n",
    "data), you can extract regularly spaced (possibly overlapping) spans of data\n",
    "by creating events using `mne.make_fixed_length_events` and then proceeding\n",
    "with epoching as described in the next section.\n",
    "\n",
    "\n",
    "\n",
    "## Epoching continuous data\n",
    "\n",
    "The `~mne.io.Raw` object and the events array are the bare minimum needed to\n",
    "create an `~mne.Epochs` object, which we create with the `~mne.Epochs` class\n",
    "constructor. Here we'll also specify some data quality constraints: we'll\n",
    "reject any epoch where peak-to-peak signal amplitude is beyond reasonable\n",
    "limits for that channel type. This is done with a *rejection dictionary*; you\n",
    "may include or omit thresholds for any of the channel types present in your\n",
    "data. The values given here are reasonable for this particular dataset, but\n",
    "may need to be adapted for different hardware or recording conditions. For a\n",
    "more automated approach, consider using the `autoreject package`_.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "reject_criteria = dict(mag=4000e-15,     # 4000 fT\n",
    "                       grad=4000e-13,    # 4000 fT/cm\n",
    "                       eeg=150e-6,       # 150 µV\n",
    "                       eog=250e-6)       # 250 µV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also pass the event dictionary as the ``event_id`` parameter (so we can\n",
    "work with easy-to-pool event labels instead of the integer event IDs), and\n",
    "specify ``tmin`` and ``tmax`` (the time relative to each event at which to\n",
    "start and end each epoch). As mentioned above, by default `~mne.io.Raw` and\n",
    "`~mne.Epochs` data aren't loaded into memory (they're accessed from disk only\n",
    "when needed), but here we'll force loading into memory using the\n",
    "``preload=True`` parameter so that we can see the results of the rejection\n",
    "criteria being applied:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "319 matching events found\n",
      "Setting baseline interval to [-0.19979521315838786, 0.0] sec\n",
      "Applying baseline correction (mode: mean)\n",
      "Created an SSP operator (subspace dimension = 4)\n",
      "4 projection items activated\n",
      "Using data from preloaded Raw for 319 events and 106 original time points ...\n",
      "    Rejecting  epoch based on EEG : ['EEG 001', 'EEG 002', 'EEG 003', 'EEG 007']\n",
      "    Rejecting  epoch based on EOG : ['EOG 061']\n",
      "    Rejecting  epoch based on MAG : ['MEG 1711']\n",
      "    Rejecting  epoch based on EOG : ['EOG 061']\n",
      "    Rejecting  epoch based on EOG : ['EOG 061']\n",
      "    Rejecting  epoch based on MAG : ['MEG 1711']\n",
      "    Rejecting  epoch based on EEG : ['EEG 008']\n",
      "    Rejecting  epoch based on EOG : ['EOG 061']\n",
      "    Rejecting  epoch based on EOG : ['EOG 061']\n",
      "    Rejecting  epoch based on EOG : ['EOG 061']\n",
      "10 bad epochs dropped\n"
     ]
    }
   ],
   "source": [
    "epochs = mne.Epochs(raw, events, event_id=event_dict, tmin=-0.2, tmax=0.5,\n",
    "                    reject=reject_criteria, preload=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll pool across left/right stimulus presentations so we can compare\n",
    "auditory versus visual responses. To avoid biasing our signals to the left or\n",
    "right, we'll use `~mne.Epochs.equalize_event_counts` first to randomly sample\n",
    "epochs from each condition to match the number of epochs present in the\n",
    "condition with the fewest good epochs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 7 epochs: 121, 195, 258, 271, 273, 274, 275\n"
     ]
    }
   ],
   "source": [
    "conds_we_care_about = ['auditory/left', 'auditory/right',\n",
    "                       'visual/left', 'visual/right']\n",
    "epochs.equalize_event_counts(conds_we_care_about)  # this operates in-place\n",
    "aud_epochs = epochs['auditory']\n",
    "vis_epochs = epochs['visual']\n",
    "del raw, epochs  # free up memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like `~mne.io.Raw` objects, `~mne.Epochs` objects also have a number of\n",
    "built-in plotting methods. One is `~mne.Epochs.plot_image`, which shows each\n",
    "epoch as one row of an image map, with color representing signal magnitude;\n",
    "the average evoked response and the sensor location are shown below the\n",
    "image:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "136 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "136 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<Figure size 1280x960 with 4 Axes>, <Figure size 1280x960 with 4 Axes>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aud_epochs.plot_image(picks=['MEG 1332', 'EEG 021'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>Note</h4><p>Both `~mne.io.Raw` and `~mne.Epochs` objects have `~mne.Epochs.get_data`\n",
    "    methods that return the underlying data as a\n",
    "    :class:`NumPy array <numpy.ndarray>`. Both methods have a ``picks``\n",
    "    parameter for subselecting which channel(s) to return; ``raw.get_data()``\n",
    "    has additional parameters for restricting the time domain. The resulting\n",
    "    matrices have dimension ``(n_channels, n_times)`` for `~mne.io.Raw` and\n",
    "    ``(n_epochs, n_channels, n_times)`` for `~mne.Epochs`.</p></div>\n",
    "\n",
    "## Time-frequency analysis\n",
    "\n",
    "The :mod:`mne.time_frequency` submodule provides implementations of several\n",
    "algorithms to compute time-frequency representations, power spectral density,\n",
    "and cross-spectral density. Here, for example, we'll compute for the auditory\n",
    "epochs the induced power at different frequencies and times, using Morlet\n",
    "wavelets. On this dataset the result is not especially informative (it just\n",
    "shows the evoked \"auditory N100\" response); see `here\n",
    "<inter-trial-coherence>` for a more extended example on a dataset with richer\n",
    "frequency content.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing projector <Projection | PCA-v1, active : True, n_channels : 102>\n",
      "Removing projector <Projection | PCA-v2, active : True, n_channels : 102>\n",
      "Removing projector <Projection | PCA-v3, active : True, n_channels : 102>\n",
      "Removing projector <Projection | Average EEG reference, active : True, n_channels : 60>\n",
      "Removing projector <Projection | PCA-v1, active : True, n_channels : 102>\n",
      "Removing projector <Projection | PCA-v2, active : True, n_channels : 102>\n",
      "Removing projector <Projection | PCA-v3, active : True, n_channels : 102>\n",
      "Removing projector <Projection | Average EEG reference, active : True, n_channels : 60>\n",
      "No baseline correction applied\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 363 out of 363 | elapsed:    2.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<Figure size 1280x960 with 2 Axes>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequencies = np.arange(7, 30, 3)\n",
    "power = mne.time_frequency.tfr_morlet(aud_epochs, n_cycles=2, return_itc=False,\n",
    "                                      freqs=frequencies, decim=3)\n",
    "power.plot(['MEG 1332'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating evoked responses\n",
    "\n",
    "Now that we have our conditions in ``aud_epochs`` and ``vis_epochs``, we can\n",
    "get an estimate of evoked responses to auditory versus visual stimuli by\n",
    "averaging together the epochs in each condition. This is as simple as calling\n",
    "the `~mne.Epochs.average` method on the `~mne.Epochs` object, and then using\n",
    "a function from the :mod:`mne.viz` module to compare the global field power\n",
    "for each sensor type of the two `~mne.Evoked` objects:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple channel types selected, returning one figure per type.\n",
      "combining channels using \"gfp\"\n",
      "combining channels using \"gfp\"\n",
      "combining channels using \"gfp\"\n",
      "combining channels using \"gfp\"\n",
      "combining channels using \"gfp\"\n",
      "combining channels using \"gfp\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<Figure size 1600x1200 with 2 Axes>,\n",
       " <Figure size 1600x1200 with 2 Axes>,\n",
       " <Figure size 1600x1200 with 2 Axes>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aud_evoked = aud_epochs.average()\n",
    "vis_evoked = vis_epochs.average()\n",
    "\n",
    "mne.viz.plot_compare_evokeds(dict(auditory=aud_evoked, visual=vis_evoked),\n",
    "                             legend='upper left', show_sensors='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get a more detailed view of each `~mne.Evoked` object using other\n",
    "plotting methods such as `~mne.Evoked.plot_joint` or\n",
    "`~mne.Evoked.plot_topomap`. Here we'll examine just the EEG channels, and see\n",
    "the classic auditory evoked N100-P200 pattern over dorso-frontal electrodes,\n",
    "then plot scalp topographies at some additional arbitrary times:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projections have already been applied. Setting proj attribute to True.\n",
      "Removing projector <Projection | PCA-v1, active : True, n_channels : 102>\n",
      "Removing projector <Projection | PCA-v2, active : True, n_channels : 102>\n",
      "Removing projector <Projection | PCA-v3, active : True, n_channels : 102>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<MNEFigure size 1800x440 with 6 Axes>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aud_evoked.plot_joint(picks='eeg')\n",
    "aud_evoked.plot_topomap(times=[0., 0.08, 0.1, 0.12, 0.2], ch_type='eeg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evoked objects can also be combined to show contrasts between conditions,\n",
    "using the `mne.combine_evoked` function. A simple difference can be\n",
    "generated by passing ``weights=[1, -1]``. We'll then plot the difference wave\n",
    "at each sensor using `~mne.Evoked.plot_topo`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing projector <Projection | Average EEG reference, active : True, n_channels : 60>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1280x960 with 1 Axes>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evoked_diff = mne.combine_evoked([aud_evoked, vis_evoked], weights=[1, -1])\n",
    "evoked_diff.pick_types(meg='mag').plot_topo(color='r', legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse modeling\n",
    "\n",
    "Finally, we can estimate the origins of the evoked activity by projecting the\n",
    "sensor data into this subject's :term:`source space` (a set of points either\n",
    "on the cortical surface or within the cortical volume of that subject, as\n",
    "estimated by structural MRI scans). MNE-Python supports lots of ways of doing\n",
    "this (dynamic statistical parametric mapping, dipole fitting, beamformers,\n",
    "etc.); here we'll use minimum-norm estimation (MNE) to generate a continuous\n",
    "map of activation constrained to the cortical surface. MNE uses a linear\n",
    ":term:`inverse operator` to project EEG+MEG sensor measurements into the\n",
    "source space. The inverse operator is computed from the\n",
    ":term:`forward solution` for this subject and an estimate of `the\n",
    "covariance of sensor measurements <tut-compute-covariance>`. For this\n",
    "tutorial we'll skip those computational steps and load a pre-computed inverse\n",
    "operator from disk (it's included with the `sample data\n",
    "<sample-dataset>`). Because this \"inverse problem\" is underdetermined (there\n",
    "is no unique solution), here we further constrain the solution by providing a\n",
    "regularization parameter specifying the relative smoothness of the current\n",
    "estimates in terms of a signal-to-noise ratio (where \"noise\" here is akin to\n",
    "baseline activity level across all of cortex).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# load inverse operator\n",
    "inverse_operator_file = (sample_data_folder / 'MEG' / 'sample' /\n",
    "                         'sample_audvis-meg-oct-6-meg-inv.fif')\n",
    "inv_operator = mne.minimum_norm.read_inverse_operator(inverse_operator_file)\n",
    "# set signal-to-noise ratio (SNR) to compute regularization parameter (λ²)\n",
    "snr = 3.\n",
    "lambda2 = 1. / snr ** 2\n",
    "# generate the source time course (STC)\n",
    "stc = mne.minimum_norm.apply_inverse(vis_evoked, inv_operator,\n",
    "                                     lambda2=lambda2,\n",
    "                                     method='MNE')  # or dSPM, sLORETA, eLORETA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, in order to plot the source estimate on the subject's cortical\n",
    "surface we'll also need the path to the sample subject's structural MRI files\n",
    "(the ``subjects_dir``):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# path to subjects' MRI files\n",
    "subjects_dir = sample_data_folder / 'subjects'\n",
    "# plot the STC\n",
    "stc.plot(initial_time=0.1, hemi='split', views=['lat', 'med'],\n",
    "         subjects_dir=subjects_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remaining tutorials have *much more detail* on each of these topics (as\n",
    "well as many other capabilities of MNE-Python not mentioned here:\n",
    "connectivity analysis, encoding/decoding models, lots more visualization\n",
    "options, etc). Read on to learn more!\n",
    "\n",
    ".. LINKS\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mne",
   "language": "python",
   "name": "mne"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
